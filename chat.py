#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
Chat + produce ç¼–æ’æœºå™¨äººï¼ˆå•æ–‡ä»¶ç‰ˆï¼‰
- è‡ªç”±èŠå¤© -> è¯†åˆ«â€œä¸»è¦é—®é¢˜â€ -> å”¤é†’ feature_engine.produce
- æˆªè·å¹¶æ”¹å†™ produce çš„æé—®ï¼Œå‘ç”¨æˆ·å‹å¥½åœ°é—®
- è‹¥ä¸Šä¸‹æ–‡å¯ç›´æ¥å›ç­”ï¼Œåˆ™ä¸å†è¿½é—®
- å¤šé€‰é¡¹ï¼šå°‘é‡é›†ä¸­é—®ï¼›é€‰é¡¹å¾ˆå¤šæ—¶é€é¡¹ç¡®è®¤
- å…¨é‡ I/O ç»Ÿä¸€æ—¥å¿—
"""

import os, re, sys, json, time, datetime, threading, queue, signal
from dataclasses import dataclass, field
from typing import List, Dict, Optional, Tuple, Any
import dotenv

dotenv.load_dotenv()

# === LLM å®¢æˆ·ç«¯ï¼ˆOpenAI å…¼å®¹ï¼‰ ============================================
try:
    from openai import OpenAI
except Exception:
    OpenAI = None

# === äº¤äº’å­è¿›ç¨‹ï¼ˆç”¨ pexpect é©±åŠ¨ produceï¼‰ ===============================
try:
    import pexpect
except Exception as e:
    print("ç¼ºå°‘ä¾èµ– pexpectï¼Œè¯·å®‰è£…ï¼špip install pexpect", file=sys.stderr)
    raise

# === æ—¥å¿—å·¥å…· =============================================================
def now():
    return datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")

def ensure_dir(p):
    os.makedirs(p, exist_ok=True)

class Logger:
    def __init__(self, path: str):
        ensure_dir(os.path.dirname(path))
        self.path = path
        self._lock = threading.Lock()

    def log(self, who: str, text: str):
        line = f"[{now()}] [{who}] {text.rstrip()}\n"
        with self._lock:
            with open(self.path, "a", encoding="utf-8") as f:
                f.write(line)

# === é…ç½® ==================================================================
@dataclass
class Config:
    # LLM
    openai_api_key: str = os.getenv("OPENAI_API_KEY", "")
    # å…¼å®¹ä¸¤ç§å†™æ³•ï¼šOPENAI_BASE_URL ä¼˜å…ˆï¼›æ²¡æœ‰åˆ™ç”¨ OPENAI_API_BASE_URL
    openai_base_url: Optional[str] = os.getenv("OPENAI_BASE_URL") or os.getenv("OPENAI_API_BASE_URL")
    openai_model: str = os.getenv("OPENAI_MODEL", "gpt-4o-mini")
    # å•ç‹¬ç»™è‡ªç”±èŠå¤©ä¸€ä¸ªæ¨¡å‹åï¼ˆå¯ä¸ä¸Šé¢ä¸€è‡´ï¼‰
    openai_chat_model: str = os.getenv("OPENAI_MODEL_CHAT", "gpt-4o-mini")

    # produce å‘½ä»¤
    produce_cmd: List[str] = field(default_factory=lambda: [
        sys.executable, "-m", "feature_engine.produce",
        "--tree", "feature_engine/nodes_trained_v2.json",
    ])

    # æç¤ºè¯­åŒ¹é…ï¼ˆæŒ‰ä½ çš„è¾“å‡ºä¿ç•™ä¸å˜æˆ–å¾®è°ƒï¼‰
    pat_main_issue: re.Pattern = re.compile(r"è¯·æè¿°ä¸»è¦é—®é¢˜ï¼ˆ[^ï¼‰]*ï¼‰\s*:\s*$")
    pat_many_features: re.Pattern = re.compile(r"æ£€æµ‹åˆ°å¤šä¸ªå¯èƒ½çš„å­ç‰¹å¾ï¼Œè¯·é€‰æ‹©å…¶ä¸€ï¼š")
    pat_enter_index: re.Pattern = re.compile(r"è¯·è¾“å…¥åºå·ï¼ˆ[^ï¼‰]*ï¼‰[^ï¼š]*ï¼š\s*$")
    pat_need_judge: re.Pattern = re.compile(r"éœ€è¦åˆ¤æ–­ï¼š(.+?)\s*ä½ çš„å›ç­”ï¼š\s*$")
    pat_next_node: re.Pattern = re.compile(r"^â¡ï¸ ä¸‹ä¸€èŠ‚ç‚¹ï¼š")
    pat_into_feature: re.Pattern = re.compile(r"^ğŸ“Œ è¿›å…¥ç‰¹å¾:")
    pat_into_problem: re.Pattern = re.compile(r"^ğŸ“Œ è¿›å…¥é—®é¢˜:")
    pat_into_solution: re.Pattern = re.compile(r"^ğŸ›  æ‰§è¡Œè§£å†³æ–¹æ¡ˆ:")
    pat_need_judge_head: re.Pattern = re.compile(r"^ğŸ”?\s*éœ€è¦åˆ¤æ–­ï¼š(.+)")
    pat_need_judge_answer: re.Pattern = re.compile(r"^ä½ çš„å›ç­”ï¼š\s*$")
    many_option_threshold: int = 5
    polite_prefix: str = "è¯·æ’æŸ¥ä»¥ä¸‹ç‰¹å¾ï¼Œè®©æˆ‘å¯ä»¥å¸®æ‚¨å®šä½é—®é¢˜"
    log_dir: str = "logs"

# === LLM å±‚ ===============================================================
class LLM:
    def __init__(self, cfg: Config, logger: Logger):
        self.cfg = cfg
        self.logger = logger
        if not cfg.openai_api_key and OpenAI is not None:
            self.logger.log("SYS", "æœªè®¾ç½® OPENAI_API_KEYï¼Œåç»­ LLM è°ƒç”¨å°†å¤±è´¥ã€‚")
        if OpenAI is not None:
            self.client = OpenAI(
                api_key=cfg.openai_api_key or None,
                base_url=cfg.openai_base_url or None
            )
        else:
            self.client = None

    def _chat(self, messages: List[Dict[str, str]], response_format: Optional[dict]=None, model: Optional[str]=None) -> str:
        self.logger.log("DEBUG", f"è¯·æ±‚ LLM: {json.dumps(messages, ensure_ascii=False)}")
        if self.client is None:
            raise RuntimeError("OpenAI å®¢æˆ·ç«¯ä¸å¯ç”¨ï¼Œè¯·å®‰è£… openai>=1.0 å¹¶è®¾ç½® OPENAI_API_KEYã€‚")
        try:
            resp = self.client.chat.completions.create(
                model=model or self.cfg.openai_model,
                messages=messages,
                temperature=0.5,
                response_format=response_format,
                timeout=30
            )
            text = resp.choices[0].message.content or ""
            self.logger.log("LLM", text)
            return text
        except Exception as e:
            self.logger.log("ERR", f"LLM è°ƒç”¨å¤±è´¥: {e}")
            # å…œåº•ï¼šè¿”å›ç©ºä¸²ï¼Œé¿å…ç¨‹åºå´©
            return ""

    def ask_natural_yesno(self, raw: str, chat_history: Optional[List[Dict[str, str]]] = None) -> str:
        """
        å°†æŠ€æœ¯åŒ–æç¤ºæ”¹å†™æˆè‡ªç„¶ä¸­æ–‡çš„ä¸€å¥â€œæ˜¯/å¦â€é—®å¥ã€‚
        ä¾‹ï¼šraw='è¯¥ç‰¹å¾æ˜¯å¦ä¸ºæ­£ï¼Ÿ(PDAæœªè¿æ¥æ­£ç¡®çš„ç½‘ç»œ)'
            -> 'PDAç°åœ¨æ˜¯ä¸æ˜¯æ²¡æœ‰è¿åˆ°æ­£ç¡®çš„ç½‘ç»œï¼Ÿï¼ˆæ˜¯/å¦ï¼‰'
        è¦æ±‚ï¼šä¸€å¥è¯ã€å£è¯­åŒ–ã€ç»“å°¾åŠ ï¼ˆæ˜¯/å¦ï¼‰ï¼Œä¸è¦â€œè¯¥ç‰¹å¾æ˜¯å¦ä¸ºæ­£â€è¿™ç§ç”Ÿç¡¬æªè¾ã€‚
        """
        ctx = (chat_history or [])[-6:]
        sys_prompt = (
            "æŠŠæŠ€æœ¯åŒ–ã€ç³»ç»ŸåŒ–çš„åˆ¤æ–­æç¤ºæ”¹å†™æˆè‡ªç„¶ä¸­æ–‡çš„ä¸€å¥æ˜¯/å¦é—®å¥ã€‚"
            "é¿å…ä½¿ç”¨â€œè¯¥ç‰¹å¾æ˜¯å¦ä¸ºæ­£â€ç­‰ç”Ÿç¡¬è¡¨è¿°ï¼›ä¿ç•™å…³é”®åè¯ï¼›å£è¯­åŒ–ï¼›å¥æœ«åŠ ï¼ˆæ˜¯/å¦ï¼‰ã€‚"
            "åªè¾“å‡ºæœ€ç»ˆé—®å¥ï¼Œä¸è¦å¤šä½™è¯´æ˜ã€‚"
            "è¯·æ³¨æ„ï¼Œä½ æ˜¯ä¸€å°å¯»æ‰¾æ•…éšœçš„æœºå™¨äºº"
        )
        payload = {"raw": raw, "history": ctx}
        q = self._chat(
            [{"role":"system","content":sys_prompt},
            {"role":"user","content":json.dumps(payload, ensure_ascii=False)}],
            model=self.cfg.openai_chat_model
        )
        q = (q or "").strip()
        # å…œåº•
        if not q:
            q = f"{raw}ï¼Ÿï¼ˆæ˜¯/å¦ï¼‰"
        return q


    def choose_or_ask(self, chat_history: List[Dict[str, str]], options: List[str]) -> Dict[str, Any]:
        """
        è®© LLM å†³å®šä¸‹ä¸€æ­¥æ€ä¹ˆé—®ï¼š
        è¿”å›ç»“æ„ï¼š
        - {"action":"decide","idx":int}                         ç›´æ¥é€‰å‡ºä¸€ä¸ªæœ€å¯èƒ½çš„
        - {"action":"ask_yn","option_idx":int,"question":str}   å…ˆé—®ä¸€ä¸ªä¿¡æ¯å¢ç›Šæœ€é«˜çš„æ˜¯/å¦
        - {"action":"ask_open","question":str}                  è®©ç”¨æˆ·å†å…·ä½“æè¿°
        - {"action":"ask_select","question":str}                ä¸€æ¬¡æ€§åˆ—çŸ­æ¸…å•è¯·ç”¨æˆ·é€‰
        """
        sys_prompt = (
            "ä½ æ˜¯äº¤äº’è§„åˆ’å™¨ã€‚æ ¹æ®å¯¹è¯å†å²ä¸å€™é€‰é¡¹ï¼ˆ<=20æ¡ï¼‰ï¼Œå†³å®šä¸‹ä¸€æ­¥å¦‚ä½•æé—®ä»¥æœ€å¿«ä¸‰æ­¥å†…ç¡®å®šç­”æ¡ˆã€‚\n"
            "ä¼˜å…ˆçº§ï¼šè‹¥æŠŠæ¡è¶³å¤Ÿâ†’ç›´æ¥å†³å®š(decide)ï¼›å¦åˆ™é€‰ä¸€æ¡æœ€åŒºåˆ†çš„ä¿¡æ¯åšæ˜¯/å¦ç¡®è®¤(ask_yn)ï¼›"
            "è‹¥ç”¨æˆ·æè¿°è¿‡å°‘â†’ask_openï¼›è‹¥é€‰é¡¹å¾ˆå°‘(â‰¤4)â†’ask_selectã€‚\n"
            "è¾“å‡ºJSONï¼š{"
            "\"action\":\"decide|ask_yn|ask_open|ask_select\","
            "\"idx\":intå¯é€‰,"
            "\"option_idx\":intå¯é€‰,"
            "\"question\":\"è‡ªç„¶ä¸­æ–‡ï¼Œä¸€å¥\"}"
        )
        payload = {
            "history": chat_history[-16:],
            "options": options
        }
        out = self._chat(
            [{"role":"system","content":sys_prompt},
            {"role":"user","content":json.dumps(payload, ensure_ascii=False)}],
            response_format={"type":"json_object"}
        )
        try:
            data = json.loads(out) if out else {}
            return data if isinstance(data, dict) else {}
        except Exception:
            return {}

    def interpret_yes_no(self, text: str) -> Optional[bool]:
        """æŠŠç”¨æˆ·è‡ªç„¶è¯­è¨€è§£ææˆæ˜¯/å¦ï¼›è¿”å› True/False/None"""
        t = text.strip().lower()
        yes = {"æ˜¯","yes","y","å¥½","å¥½çš„","å¯¹","å¯¹çš„","å—¯","ok","å¯ä»¥","å·²è§£å†³","è§£å†³","å¥½äº†"}
        no  = {"å¦","no","n","ä¸æ˜¯","ä¸å¯¹","è¿˜æ²¡","æ²¡æœ‰","æœªè§£å†³","ä¸è¡Œ","æ²¡å¥½"}
        if t in yes: return True
        if t in no:  return False
        # äº¤ç»™LLMå…œåº•
        sys_prompt = 'å°†ç”¨æˆ·å›å¤åˆ¤å®šä¸º"yes"æˆ–"no"æˆ–"unsure"ï¼Œåªè¾“å‡ºå…¶ä¸­ä¸€ä¸ªå•è¯ã€‚'
        out = self._chat(
            [{"role":"system","content":sys_prompt},
            {"role":"user","content":text}],
        )
        out = (out or "").strip().lower()
        if "yes" in out or "æ˜¯" in out: return True
        if "no" in out or "å¦" in out:  return False
        return None

    # æ˜¯å¦å·²æè¿°ä¸»è¦é—®é¢˜
    def detect_main_issue_from_history(self, chat_history: List[Dict[str, str]]) -> Dict[str, Any]:
        sys_prompt = (
            "ä½ æ˜¯ä¸€ä¸ªä¸¥æ ¼çš„ä¿¡æ¯æŠ½å–å™¨ã€‚æ ¹æ®æœ€è¿‘çš„å¯¹è¯å†å²ï¼Œåˆ¤æ–­ç”¨æˆ·æ˜¯å¦å·²ç»æ˜ç¡®è¯´å‡ºäº†ä¸»è¦é—®é¢˜ã€‚"
            "è¾“å‡º JSONï¼š{\"has_issue\": true|false, \"issue_text\": \"ç®€çŸ­é—®é¢˜çŸ­è¯­æˆ–ç©º\"}ã€‚"
            "ä¸»è¦é—®é¢˜ä¾‹ï¼šæœºå™¨äººå¼€ä¸äº†æœº/æ— æ³•ç§»åŠ¨/æ— æ³•å……ç”µ/RCSæ˜¾ç¤ºç”µé‡ä½/æ‰€æœ‰å°è½¦æ— æ³•å¼€æœº ç­‰ã€‚"
            "è‹¥è¡¨è¿°åˆ†æ•£è¯·å½’çº³ä¸ºæœ€æ¥è¿‘çš„ä¸€æ¡ã€‚"
        )
        msgs = [{"role":"system","content":sys_prompt}]
        msgs += chat_history[-20:]  # å–è¿‘ 20 æ¡
        out = self._chat(msgs, response_format={"type":"json_object"})
        try:
            return json.loads(out) if out else {"has_issue": False, "issue_text": ""}
        except Exception:
            return {"has_issue": False, "issue_text": ""}

    # â€”â€” æ”¹è‰¯è‡ªç”±èŠå¤©ï¼šè‡ªç„¶å¯¹è¯ + å¼•å¯¼æŒ–æ˜å…³é”®ä¿¡æ¯ â€”â€”
    def smart_reply(self, chat_history: List[Dict[str, str]]) -> str:
        sys_prompt = (
            "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šè€Œè‡ªç„¶çš„ä¸­æ–‡æŠ€æœ¯å®¢æœå¯¹è¯åŠ©æ‰‹ã€‚"
            "ç›®æ ‡ï¼šç”¨ç®€æ´è‡ªç„¶çš„è¯­æ°”å¯¹è¯ï¼Œæ¯æ¬¡æœ€å¤š2å¥ï¼ŒåŒæ—¶æå‡º1ä¸ªå…·ä½“è¿½é—®ï¼Œå¸®åŠ©å°½å¿«é”å®šä¸»è¦é—®é¢˜ã€‚"
            "è¿½é—®å»ºè®®ä¼˜å…ˆçº§ï¼šç°è±¡ç»†èŠ‚â†’è®¾å¤‡å‹å·/æ•°é‡â†’æ˜¯å¦æ‰¹é‡/å•å°â†’æ˜¯å¦æœ‰æŠ¥é”™æç¤º/ç¯å…‰â†’æœ€è¿‘æ”¹åŠ¨â†’èƒ½å¦ç¨³å®šå¤ç°ã€‚"
            "é¿å…é‡å¤åŒä¸€å¥è¯ï¼›é¿å…æœºæ¢°å£å»ã€‚"
        )
        msgs = [{"role":"system","content":sys_prompt}]
        msgs += chat_history[-12:]  # å¼•å…¥æœ€è¿‘ä¸Šä¸‹æ–‡
        return self._chat(msgs, model=self.cfg.openai_chat_model)

    # â€”â€” åŸæœ‰ detect_main_issueï¼ˆä¿ç•™ï¼Œç”¨äºå•å¥åœºæ™¯ï¼‰ â€”â€”
    def detect_main_issue(self, user_utterance: str) -> Dict[str, Any]:
        sys_prompt = (
            "ä½ æ˜¯ä¸€ä¸ªä¸¥æ ¼çš„æŠ½å–å™¨ã€‚è¾“å…¥ä¸ºç”¨æˆ·æœ€æ–°ä¸€å¥è¯ï¼Œè¾“å‡º JSONï¼š"
            '{"has_issue": true|false, "issue_text": "è‹¥å·²è¡¨è¿°ä¸»è¦é—®é¢˜åˆ™æŠ½å–ç®€çŸ­çŸ­è¯­ï¼Œå¦åˆ™ç©º"}'
        )
        user_prompt = f"ç”¨æˆ·ï¼š{user_utterance}\nä»…è¾“å‡º JSONï¼š"
        out = self._chat(
            [{"role": "system", "content": sys_prompt},
             {"role": "user", "content": user_prompt}],
            response_format={"type":"json_object"}
        )
        try:
            return json.loads(out) if out else {"has_issue": False, "issue_text": ""}
        except Exception:
            return {"has_issue": False, "issue_text": ""}

    def prettify_question(self, raw: str, options: Optional[List[str]]=None) -> Dict[str, Any]:
        sys_prompt = (
            "ä½ æ˜¯ä¸­æ–‡äº§å“åŠ©ç†ã€‚å°†æŠ€æœ¯æ€§æç¤ºæ”¹å†™ä¸ºç”¨æˆ·å‹å¥½çš„å•é—®å¥ï¼›è‹¥æœ‰é€‰é¡¹ï¼Œ"
            "è¯·ç»™å‡ºç®€æ´çš„é€‰æ‹©è¯´æ˜ã€‚è¾“å‡º JSONï¼š"
            '{"ask":"é—®é¢˜å¥","mode":"single|multi|yn","options":["..",".."]}'
        )
        payload = {"raw": raw, "options": options or []}
        out = self._chat(
            [{"role":"system","content":sys_prompt},
             {"role":"user","content":json.dumps(payload, ensure_ascii=False)}],
            response_format={"type":"json_object"}
        )
        try:
            obj = json.loads(out)
            if options:
                obj["options"] = options
            return obj
        except Exception:
            # å…œåº•
            return {"ask": raw, "mode":"single", "options": options or []}

    # â€”â€” æ–°å¢ï¼šä»å†å²æŠ½å–â€œå·²çŸ¥äº‹å®â€ï¼Œä¾›åç»­è‡ªåŠ¨å›å¡« produce â€”â€”
    def extract_facts(self, chat_history: List[Dict[str,str]]) -> Dict[str, Any]:
        sys_prompt = (
            "æŠ½å–ä¸è®¾å¤‡æ•…éšœç›¸å…³çš„äº‹å®ï¼Œè¾“å‡º JSONï¼š"
            '{"features": ["æåˆ°çš„å…³é”®ç°è±¡æˆ–ç‰¹å¾(å¦‚RCSæ˜¾ç¤ºç”µé‡ä½, APç¦»çº¿, é˜²ç«å¢™æœªå¼€ç­‰)"],'
            ' "resolved": "yes|no|unsure"}'
        )
        msgs = [{"role":"system","content":sys_prompt}]
        msgs += chat_history[-30:]
        out = self._chat(msgs, response_format={"type":"json_object"})
        try:
            data = json.loads(out) if out else {}
            data.setdefault("features", [])
            data.setdefault("resolved", "unsure")
            return data
        except Exception:
            return {"features": [], "resolved": "unsure"}
        
    def infer_answer_from_context(self, chat_history: List[Dict[str,str]], raw_prompt: str, options: Optional[List[str]] = None) -> Dict[str, Any]:
        sys_prompt = (
            """
            ä½ æ˜¯ä¸€ä¸ªä¸¥æ ¼çš„åˆ¤æ–­å™¨ã€‚
            - ä»…å½“ç”¨æˆ·å†å²æ–‡æœ¬é‡Œå‡ºç°æŸä¸ªé€‰é¡¹é‡Œçš„å…³é”®è¯æˆ–åŒä¹‰è¯æ—¶ï¼Œæ‰è¾“å‡º {"can_answer": true, "answer": "<è¯¥é€‰é¡¹æ–‡æœ¬æˆ–åºå·>"}ã€‚
            - å¦‚æœæ²¡æœ‰å‡ºç°ï¼Œå¿…é¡»è¾“å‡º {"can_answer": false, "answer": ""}ã€‚
            - ä¸è¦æ¨ç†ï¼Œä¸è¦è”æƒ³ï¼Œä¸è¦è§£é‡Šã€‚
            - å¿…é¡»è¿”å›json æ ¼å¼
            """
        )
        payload = {
            "raw_prompt": raw_prompt,
            "options": options or [],
            "history": chat_history[-20:]  # å–è¿‘ 20 è½®
        }
        out = self._chat(
            [{"role":"system","content":sys_prompt},
             {"role":"user","content":json.dumps(payload, ensure_ascii=False)}],
            response_format={"type":"json_object"}
        )
        self.logger.log("DEBUG", f"infer_answer_from_context: {out}")
        try:
            return json.loads(out)
        except Exception:
            return {"can_answer": False, "answer": ""}
        
# === produce äº¤äº’æ¡¥ =======================================================
class ProduceBridge:
    def __init__(self, cfg: Config, logger: Logger):
        self.cfg = cfg
        self.logger = logger
        self.child: Optional[pexpect.spawn] = None
        self.alive = False

    def start(self):
        cmd = " ".join(self.cfg.produce_cmd)
        self.logger.log("SYS", f"å¯åŠ¨ produce: {cmd}")
        self.child = pexpect.spawn(
            command=self.cfg.produce_cmd[0],
            args=self.cfg.produce_cmd[1:],
            encoding="utf-8",
            timeout=None
        )
        self.alive = True

    def stop(self):
        if self.child and self.alive:
            try:
                self.child.sendcontrol('c')
            except Exception:
                pass
            try:
                self.child.terminate(force=True)
            except Exception:
                pass
        self.alive = False

    def expect_and_read_until_prompt(self, linger: float = 0.8, max_wait: float = 5.0):
        """
        è¿ç»­æ‹‰å–å­è¿›ç¨‹è¾“å‡ºï¼Œç›´åˆ°ï¼š
        - å‘ç°è¾“å…¥æç¤ºï¼ˆä»¥â€œï¼šâ€æ”¶å°¾æˆ–å‘½ä¸­å·²çŸ¥æç¤ºæ­£åˆ™ï¼‰ï¼Œæˆ–è€…
        - åœ¨ linger ç§’å†…æ²¡æœ‰ä»»ä½•æ–°è¾“å‡ºï¼Œæˆ–è€…
        - è¶…è¿‡ max_wait æ€»ç­‰å¾…æ—¶é—´
        è¿”å›: (lines, saw_prompt)
        """
        lines = []
        if not self.alive or not self.child:
            return lines, False

        def is_prompt_line(ln: str) -> bool:
            ln = ln.rstrip()
            if ln.endswith("ï¼š"):
                return True
            if self.cfg.pat_enter_index.search(ln):
                return True
            if self.cfg.pat_need_judge.search(ln):
                return True
            if self.cfg.pat_many_features.search(ln):
                return True
            return False

        start = time.time()
        last_activity = time.time()
        saw_prompt = False

        while True:
            try:
                s = self.child.read_nonblocking(size=4096, timeout=0.1)
                if s:
                    for ln in s.splitlines():
                        self.logger.log("PRODUCE", ln)
                        lines.append(ln)
                        if is_prompt_line(ln):
                            saw_prompt = True
                    last_activity = time.time()
                    if saw_prompt:
                        break
            except pexpect.exceptions.TIMEOUT:
                pass
            except pexpect.exceptions.EOF:
                self.alive = False
                break

            if time.time() - last_activity >= linger:
                break
            if time.time() - start >= max_wait:
                break

        return lines, saw_prompt


    def sendline(self, text: str):
        if not self.alive or not self.child:
            return
        self.logger.log("TO_PRODUCE", text)
        self.child.sendline(text)

# === Orchestrator =========================================================
class Orchestrator:
    def __init__(self, cfg: Config):
        stamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
        self.logger = Logger(os.path.join(cfg.log_dir, f"session_{stamp}.log"))
        self.cfg = cfg
        self.llm = LLM(cfg, self.logger)
        self.bridge = None  # type: Optional[ProduceBridge]
        self.chat_history: List[Dict[str,str]] = []  # {"role":"user/assistant/system", "content":...}
        self.mode = "free_chat"  # or "produce"
        self.pending_question: Optional[Dict[str, Any]] = None  # å½“å‰ç­‰å¾…ç”¨æˆ·å›ç­”çš„é—®é¢˜ä¸Šä¸‹æ–‡
        self.known_facts: Dict[str, Any] = {"features": [], "resolved": "unsure"}  # â† æ–°å¢


    def _map_answer_to_index(self, answer: str, options: List[str]) -> Optional[int]:
        """
        æŠŠ LLM/ç”¨æˆ·ç»™çš„ç­”æ¡ˆæ˜ å°„ä¸ºé€‰é¡¹çš„ç´¢å¼•ï¼š
        - è‹¥æœ¬èº«æ˜¯çº¯æ•°å­—ä¸”åœ¨èŒƒå›´å†…ï¼šç›´æ¥ç”¨
        - å¦åˆ™åšå…³é”®è¯åŒ¹é…ï¼ˆåŒ…å«/ç›¸ç­‰ï¼‰ï¼Œå‘½ä¸­ç¬¬ä¸€ä¸ªå°±è¿”å›
        - éƒ½ä¸å‘½ä¸­åˆ™è¿”å› None
        """
        if answer is None:
            return None
        s = str(answer).strip()
        if s.isdigit():
            i = int(s)
            return i if 0 <= i < len(options) else None

        # è§„èŒƒåŒ–åšåŒ…å«/ç›¸ç­‰åŒ¹é…ï¼ˆä¸­è‹±æ–‡éƒ½å¯ï¼‰
        def norm(x: str) -> str:
            return re.sub(r"\s+", "", x.lower())

        s_norm = norm(s)
        # å®Œå…¨ç›¸ç­‰ä¼˜å…ˆ
        for i, opt in enumerate(options):
            if s_norm == norm(opt):
                return i
        # å­ä¸²åŒ…å«æ¬¡ä¹‹ï¼ˆâ€œrcsæ˜¾ç¤ºç”µé‡ä½â€ å‘½ä¸­ â€œRCSæ˜¾ç¤ºç”µé‡ä½â€ï¼‰
        for i, opt in enumerate(options):
            on = norm(opt)
            if s_norm and (s_norm in on or on in s_norm):
                return i
        return None

    # ---- ä¸»å›è·¯ï¼ˆCLI ç¤ºä¾‹ï¼‰ ----
    def run_cli(self):
        print("ğŸŸ¢ è¯Šæ–­èŠå¤©åŠ©æ‰‹å·²å¯åŠ¨ã€‚è¾“å…¥ Ctrl+C é€€å‡ºã€‚")
        try:
            while True:
                user = input("> ä½ ï¼š").strip()
                if user == "":
                    continue
                self.logger.log("USER", user)
                self.chat_history.append({"role":"user", "content":user})

                # â€”â€” æ–°å¢ï¼šæ¯æ¬¡ç”¨æˆ·è¯´è¯åæŠ½å–äº‹å®ï¼Œä¾¿äºè‡ªåŠ¨å›å¡« produce â€”â€”
                facts = self.llm.extract_facts(self.chat_history)
                # åˆå¹¶å»é‡
                feats = set(self.known_facts.get("features", [])) | set(facts.get("features", []))
                self.known_facts["features"] = list(feats)
                self.known_facts["resolved"] = facts.get("resolved", self.known_facts["resolved"])

                if self.pending_question:
                    answered = self.handle_user_answer_to_pending(user)
                    if answered:
                        continue

                if self.mode == "free_chat":
                    self.free_chat_step(user)
                else:
                    # è¿›å…¥ produceï¼šå…ˆæ¨è¿›ä¸€è½®
                    self.produce_step()
                    # è‹¥æ²¡æœ‰æŒ‚èµ·é—®é¢˜ï¼Œç»§ç»­çŸ­æš‚å¿«ç…§è½®è¯¢ï¼Œé¿å…â€œæ˜æ˜è¿˜èƒ½è·‘å´ç­‰äººâ€çš„å‡æŒ‚èµ·
                    if self.mode == "produce" and self.pending_question is None and self.bridge and self.bridge.alive:
                        deadline = time.time() + 100  # å¿«é€ŸæŠ½æ°´çª—å£
                        while time.time() < deadline and self.pending_question is None and self.bridge.alive:
                            time.sleep(0.3)
                            self.produce_step()

        except KeyboardInterrupt:
            print("\nğŸ‘‹ å·²é€€å‡ºã€‚")
        finally:
            if self.bridge:
                self.bridge.stop()

    # ---- è‡ªç”±èŠå¤©ï¼šæ£€æµ‹æ˜¯å¦å…·å¤‡â€œä¸»è¦é—®é¢˜â€ ----
    def free_chat_step(self, last_user_text: str):
        # å…ˆä»å¯¹è¯å†å²ç»¼åˆåˆ¤æ–­æ˜¯å¦å·²æœ‰ä¸»è¦é—®é¢˜
        det_hist = self.llm.detect_main_issue_from_history(self.chat_history)
        if det_hist.get("has_issue"):
            issue = det_hist.get("issue_text", "").strip() or last_user_text
            print(f"ğŸ¤– æ˜ç™½äº†ï¼Œä½ çš„ä¸»è¦é—®é¢˜æ˜¯ï¼š{issue}ã€‚æˆ‘æ¥å¯åŠ¨è¯Šæ–­å¼•æ“ã€‚")
            self.logger.log("ASSIST", f"è¯†åˆ«ä¸»è¦é—®é¢˜ï¼š{issue}")
            self.bridge = ProduceBridge(self.cfg, self.logger)
            self.bridge.start()
            lines, _ = self.bridge.expect_and_read_until_prompt()
            if lines and any(self.cfg.pat_main_issue.search(ln) for ln in lines):
                self.bridge.sendline(issue)
            else:
                self.bridge.sendline(issue)
            self.mode = "produce"
            self.produce_step()
            return

        # å¦åˆ™è¿›å…¥è‡ªç„¶èŠå¤©ï¼Œç»§ç»­å¼•å¯¼
        reply = self.llm.smart_reply(self.chat_history) or "æˆ‘åœ¨å¬ï½å¯ä»¥å†å¤šæè¿°ä¸€ç‚¹å…·ä½“ç°è±¡å—ï¼Ÿ"
        print(f"ğŸ¤– {reply}")
        self.logger.log("ASSIST", reply)
        self.chat_history.append({"role":"assistant","content":reply})

    # ---- ç®€å•é—²èŠå…œåº• ----
    def simple_smalltalk(self, text: str) -> str:
        # ä½ å¯æ”¹ä¸ºçœŸæ­£ LLM é—²èŠï¼›è¿™é‡Œä¿æŒç®€æ´
        return "æ”¶åˆ°ï¼Œæˆ‘ä¼šç»§ç»­è®°å½•ä½ çš„æè¿°ã€‚æœ‰æ›´å¤šç»†èŠ‚ä¹Ÿå¯ä»¥ç»§ç»­è¡¥å……ã€‚"

    # ---- produce æ¨è¿›ä¸€æ­¥ ----
    def produce_step(self):
        if not self.bridge or not self.bridge.alive:
            print("âš ï¸ è¯Šæ–­å¼•æ“å·²ç»“æŸæˆ–å¼‚å¸¸é€€å‡ºã€‚è‹¥é—®é¢˜ä»åœ¨ï¼Œæˆ‘ä»¬å¯ä»¥å†å¯åŠ¨ä¸€æ¬¡ã€‚")
            self.logger.log("SYS", "produce not alive on produce_step()")
            self.mode = "free_chat"
            return

        self.logger.log("DEBUG", "è¿›å…¥ produce_step å¾ªç¯")
        while True:
            lines, saw_prompt = self.bridge.expect_and_read_until_prompt()
            self.logger.log("DEBUG", f"produce_step: è¯»å– {len(lines)} è¡Œ, saw_prompt={saw_prompt}")


            # 1) éœ€è¦åˆ¤æ–­ï¼ˆæ˜¯/å¦ï¼‰
            need_judge_line = next((ln for ln in lines if self.cfg.pat_need_judge.search(ln)), None)
            if need_judge_line:
                m = self.cfg.pat_need_judge.search(need_judge_line)
                raw = m.group(0)
                infer = self.llm.infer_answer_from_context(self.chat_history, raw, None)
                if infer.get("can_answer"):
                    ans = (infer.get("answer","") or "").strip()
                    self.logger.log("DEBUG", f"need_judge è‡ªåŠ¨å›å¡«: {ans}")
                    self.bridge.sendline(ans)
                    continue
                # éœ€è¦ç”¨æˆ·å›ç­”
                pretty = self.llm.prettify_question(raw, None)
                ask = self.llm.ask_natural_yesno(raw, self.chat_history)
                self.pending_question = {"type": "yn_or_free", "raw": raw}
                print("ğŸ¤– " + ask)
                self.logger.log("ASSIST", f"æé—®ï¼ˆåˆ¤æ–­ï¼‰ï¼š{ask}")
                return

            judge_head_line = next((ln for ln in lines if self.cfg.pat_need_judge_head.search(ln)), None)
            has_judge_answer = any(self.cfg.pat_need_judge_answer.search(ln) for ln in lines)

            if judge_head_line and has_judge_answer:
                raw = self.cfg.pat_need_judge_head.search(judge_head_line).group(1).strip()
                # å…ˆå°è¯•ä»ä¸Šä¸‹æ–‡è‡ªåŠ¨ç»™å‡ºâ€œæ˜¯/å¦â€
                infer = self.llm.infer_answer_from_context(self.chat_history, f"éœ€è¦åˆ¤æ–­ï¼š{raw}", None)
                ans = (infer.get("answer","") or "").strip()
                yn = self.llm.interpret_yes_no(ans)
                if yn is True:
                    self.logger.log("DEBUG", f"need_judge(åˆ†è¡Œ) è‡ªåŠ¨å›å¡«: æ˜¯  [{raw}]")
                    self.bridge.sendline("æ˜¯")
                    # ç»§ç»­æ³µä¸‹ä¸€è½®
                    continue
                if yn is False:
                    self.logger.log("DEBUG", f"need_judge(åˆ†è¡Œ) è‡ªåŠ¨å›å¡«: å¦  [{raw}]")
                    self.bridge.sendline("å¦")
                    continue

                # è‡ªåŠ¨åˆ¤ä¸å‡ºæ¥ â†’ å‘ç”¨æˆ·å‘â€œæ˜¯/å¦â€ç¡®è®¤ï¼ˆè‡ªç„¶è¯­æ°”ï¼‰
                ask = self.llm.ask_natural_yesno(raw, self.chat_history)
                self.pending_question = {"type":"yn_or_free","raw":raw}
                print("ğŸ¤– " + ask)
                self.logger.log("ASSIST", f"æé—®ï¼ˆåˆ¤æ–­-åˆ†è¡Œï¼‰ï¼š{ask}")
                return


            # 2) å¤šä¸ªå­ç‰¹å¾
            many = any(self.cfg.pat_many_features.search(ln) for ln in lines)
            need_index = any(self.cfg.pat_enter_index.search(ln) for ln in lines)
            if many or need_index:
                options = []
                for ln in lines:
                    m = re.match(r"^\s*(\d+)\.\s*[A-Za-z0-9_]+:(.+)$", ln.strip())
                    if m:
                        _i, desc = m.group(1), m.group(2).strip()
                        options.append(desc)
                if options:
                    # a) å·²çŸ¥äº‹å®å‘½ä¸­â†’ç›´æ¥å›åºå·
                    facts = self.known_facts.get("features", [])
                    hit_idx = next((i for i, opt in enumerate(options) if any(k in opt for k in facts)), None)
                    if hit_idx is not None:
                        self.logger.log("DEBUG", f"known_facts å‘½ä¸­ï¼Œå›åºå· {hit_idx}")
                        self.bridge.sendline(str(hit_idx))
                        continue

                    # b) ä¸Šä¸‹æ–‡æ¨æ–­â†’æ˜ å°„ä¸ºç´¢å¼•
                    infer = self.llm.infer_answer_from_context(self.chat_history, "å¦‚æœä½ èƒ½é€‰æ‹©å­ç‰¹å¾ï¼Œè¯·é€‰æ‹©, ä¸è¦æ¨æµ‹ï¼Œå›ç­”å¿…é¡»ä¸¥è°¨", options)
                    if infer.get("can_answer"):
                        ans = (infer.get("answer","") or "").strip()
                        idx = self._map_answer_to_index(ans, options)
                        if idx is not None:
                            self.logger.log("DEBUG", f"infer å‘½ä¸­: answer={ans} -> idx={idx}")
                            self.bridge.sendline(str(idx))
                            continue

                    # c) äº¤ç»™ LLM å†³å®šé—®æ³•
                    plan = self.llm.choose_or_ask(self.chat_history, options) or {}
                    act = plan.get("action")
                    self.logger.log("DEBUG", f"probe plan: {plan}")

                    if act == "decide" and isinstance(plan.get("idx"), int) and 0 <= plan["idx"] < len(options):
                        self.bridge.sendline(str(plan["idx"]))
                        continue

                    if act == "ask_yn" and isinstance(plan.get("option_idx"), int) and 0 <= plan["option_idx"] < len(options):
                        q = plan.get("question") or f"æ˜¯å¦å­˜åœ¨ä»¥ä¸‹ç°è±¡ï¼š{options[plan['option_idx']]}ï¼Ÿï¼ˆæ˜¯/å¦ï¼‰"
                        self.pending_question = {
                            "type": "yn_probe",
                            "options": options,
                            "target_idx": plan["option_idx"],
                            "ruled_out": set()
                        }
                        print("ğŸ¤– " + q)
                        self.logger.log("ASSIST", f"æé—®ï¼ˆYNç¡®è®¤ï¼‰ï¼š{q}")
                        return

                    if act == "ask_open":
                        q = plan.get("question") or "èƒ½å†å…·ä½“æè¿°ä¸€ä¸‹æ•…éšœç°è±¡/æ—¶é—´ç‚¹/æŠ¥é”™æç¤ºå—ï¼Ÿ"
                        self.pending_question = {
                            "type": "open_probe",
                            "options": options
                        }
                        print("ğŸ¤– " + q)
                        self.logger.log("ASSIST", f"æé—®ï¼ˆå¼€æ”¾è¿½é—®ï¼‰ï¼š{q}")
                        return

                    # é»˜è®¤é€€å›å•é€‰æ¸…å•ï¼ˆâ‰¤4é¡¹æ‰åˆ—ï¼‰
                    short = options[:4]
                    msg = f"{self.cfg.polite_prefix}ä»¥ä¸‹å“ªä¸€é¡¹æœ€ç¬¦åˆï¼Ÿ\n" + \
                        "\n".join([f"{i}. {opt}" for i, opt in enumerate(short)])
                    msg += "\nï¼ˆå¯å›å¤ç¼–å·æˆ–å…³é”®å­—ï¼›è‹¥éƒ½ä¸ç¬¦åˆç›´æ¥è¯´å‡ºæ¥ï¼‰"
                    self.pending_question = {"type":"select", "options": short}
                    print("ğŸ¤– " + msg)
                    self.logger.log("ASSIST", "æé—®ï¼ˆå•é€‰/çŸ­æ¸…å•ï¼‰ï¼š\n" + msg)
                    return

            # 3) å…¶ä»–ä»¥â€œï¼šâ€æ”¶å°¾çš„é€šç”¨è¾“å…¥
            prompt_line = next((ln for ln in lines if ln.rstrip().endswith("ï¼š")), None)
            if prompt_line:
                infer = self.llm.infer_answer_from_context(self.chat_history, prompt_line, None)
                if infer.get("can_answer"):
                    ans = (infer.get("answer","") or "").strip()
                    self.logger.log("DEBUG", f"é€šç”¨æç¤ºè‡ªåŠ¨å›å¡«: {ans}")
                    self.bridge.sendline(ans)
                    continue
                pretty = self.llm.prettify_question(prompt_line, None)
                ask = pretty.get("ask") or prompt_line
                self.pending_question = {"type":"free", "raw": prompt_line}
                print(f"ğŸ¤– {self.cfg.polite_prefix}{ask}")
                self.logger.log("ASSIST", f"æé—®ï¼ˆè‡ªç”±è¾“å…¥ï¼‰ï¼š{ask}")
                return

            # 4) æ— äº¤äº’æç¤º
            if not saw_prompt:
                # ç­‰ä¸€ä¸¢ä¸¢å†æ‹‰ä¸€æ¬¡
                time.sleep(0.3)

            # self.logger.log(f"line: {lines[-1] if lines else 'æ— è¾“å‡º'}")

            # self.logger.log("DEBUG", "æœ¬è½®æ— è¾“å…¥æç¤ºï¼Œè¿”å›")
            # return

            # è‹¥æ²¡æœ‰æ–°çš„è¾“å…¥æç¤ºï¼Œåˆ™é™é»˜ç­‰å¾…ä¸‹ä¸€è½®
            # å¯åœ¨æ­¤æ‰“å° produce çš„é˜¶æ®µæ€§ä¿¡æ¯ï¼ˆå¦‚è¿›å…¥é—®é¢˜/è§£å†³æ–¹æ¡ˆç­‰ï¼‰ï¼Œè¿™é‡Œä¿æŒå®‰é™

    def ask_next_iter_option(self):
        ctx = self.pending_question
        options = ctx["options"]
        cur = ctx["cursor"]
        if cur >= len(options):
            if ctx["accepted"]:
                # accepted é‡Œæ˜¯æ–‡æœ¬ï¼ŒæŠŠå®ƒæ˜ å°„ä¸ºç´¢å¼•
                first_text = ctx["accepted"][0]
                try:
                    idx = options.index(first_text)
                    self.logger.log("DEBUG", f"iter ç»“æŸï¼Œç”¨ accepted[0] -> idx={idx}")
                    self.bridge.sendline(str(idx))
                except ValueError:
                    self.logger.log("DEBUG", f"iter ç»“æŸï¼Œä½† accepted[0] ä¸åœ¨ options ä¸­ï¼Œå›ä¼ ç©ºè¡Œ")
                    self.bridge.sendline("")  # fallback
            else:
                self.bridge.sendline("")  # è¡¨ç¤ºéƒ½ä¸æ˜¯
            return
        opt = options[cur]
        q = f"è¯·ç¡®è®¤ï¼šæ˜¯å¦å­˜åœ¨æ­¤ç°è±¡â€”â€”â€œ{opt}â€ï¼Ÿï¼ˆæ˜¯/å¦ï¼‰"
        print("ğŸ¤– " + q)
        self.logger.log("ASSIST", f"æé—®ï¼ˆè¿­ä»£ç¡®è®¤ï¼‰ï¼š{opt}")

    # ---- å°†ç”¨æˆ·è¾“å…¥æ˜ å°„åˆ° pending_question å¹¶å›å¡«ç»™ produce ----
    def handle_user_answer_to_pending(self, user_text: str) -> bool:
        if not self.pending_question:
            return False
        kind = self.pending_question["type"]

        def match_option(user_text: str, options: List[str]) -> Optional[str]:
            # ç¼–å·åŒ¹é…
            m = re.match(r"^\s*(\d+)\s*$", user_text)
            if m:
                idx = int(m.group(1))
                if 0 <= idx < len(options):
                    return str(idx)
            # å…³é”®å­—æ¨¡ç³ŠåŒ¹é…ï¼ˆæœ€é•¿å­ä¸²ä¼˜å…ˆï¼‰
            txt = user_text.strip()
            ranked = sorted(options, key=lambda o: (txt in o, len(os.path.commonprefix([txt,o]))), reverse=True)
            if txt and (txt in ranked[0]):
                # ç›´æ¥ç”¨å…³é”®å­—å›ä¼ 
                return txt
            return None

        if kind == "yn_or_free":
            # è§„èŒƒåŒ–æ˜¯/å¦/yes/no
            norm = user_text.strip().lower()
            if norm in ["æ˜¯","yes","y","å·²è§£å†³","è§£å†³","ok","å¥½äº†"]:
                self.bridge.sendline("æ˜¯")
            elif norm in ["å¦","no","n","æœªè§£å†³","æ²¡æœ‰","è¿˜æ²¡"]:
                self.bridge.sendline("å¦")
            else:
                # å›ä¼ åŸæ–‡ï¼ˆæœ‰äº› produce å…è®¸è‡ªç”±æ–‡æœ¬ï¼‰
                self.bridge.sendline(user_text)
            self.pending_question = None
            # æ¨è¿›
            self.produce_step()
            return True

        if kind == "select":
            opts = self.pending_question["options"]

            # â‘  è¾“å…¥æ˜¯æ•°å­— â†’ ç›´æ¥å›åºå·
            if user_text.strip().isdigit():
                idx = int(user_text.strip())
                if 0 <= idx < len(opts):
                    self.logger.log("DEBUG", f"user ç›´è¾“æ•°å­— idx={idx}")
                    self.bridge.sendline(str(idx))
                    self.pending_question = None
                    self.produce_step()
                    return True
                else:
                    print("ğŸ¤– è¾“å…¥çš„æ•°å­—è¶…å‡ºèŒƒå›´ï¼Œè¯·é‡æ–°è¾“å…¥ã€‚")
                    return True

            # â‘¡ å°è¯•æŠŠå…³é”®å­—æ˜ å°„ä¸ºåºå·
            idx = self._map_answer_to_index(user_text, opts)
            if idx is not None:
                self.logger.log("DEBUG", f"user å…³é”®å­—åŒ¹é… -> idx={idx}")
                self.bridge.sendline(str(idx))
                self.pending_question = None
                self.produce_step()
                return True

            # â‘¢ ç©ºè¡Œä»£è¡¨â€œéƒ½ä¸æ˜¯â€
            self.bridge.sendline("")
            self.pending_question = None
            self.produce_step()
            return True

            # print("ğŸ¤– æˆ‘æ²¡èƒ½å¯¹åº”åˆ°é€‰é¡¹ï¼Œè¯·å›å¤ç¼–å·æˆ–å…³é”®å­—ã€‚")
            # return True

        if kind == "select_many_iter":
            ctx = self.pending_question
            opts = ctx["options"]
            cur = ctx["cursor"]
            norm = user_text.strip().lower()
            if norm in ["æ˜¯","yes","y"]:
                ctx["accepted"].append(opts[cur])
            # å¦/å…¶ä»–å‡è§†ä½œâ€œä¸å­˜åœ¨â€
            ctx["cursor"] += 1
            self.ask_next_iter_option()
            # å½“è¿­ä»£ç»“æŸæ—¶ï¼Œask_next_iter_option ä¼šå›å¡«
            if ctx["cursor"] >= len(opts):
                self.pending_question = None
                # ä¸‹ä¸€æ­¥è¾“å‡ºå°†ç”± produce_step æ•è·
                time.sleep(0.05)
                self.produce_step()
            return True

        if kind == "free":
            self.bridge.sendline(user_text)
            self.pending_question = None
            self.produce_step()
            return True

        if kind == "yn_probe":
            opts = self.pending_question["options"]
            t = self.pending_question["target_idx"]
            yn = self.llm.interpret_yes_no(user_text)
            if yn is True:
                # ç¡®è®¤å‘½ä¸­ â†’ ç›´æ¥å›åºå·å¹¶ç»“æŸ probe
                self.bridge.sendline(str(t))
                self.pending_question = None
                self.produce_step()
                return True
            if yn is False:
                # å¦è®¤ â†’ ä»å€™é€‰ä¸­æ’é™¤ï¼Œé‡æ–°è§„åˆ’
                ruled = self.pending_question.get("ruled_out", set())
                ruled.add(t)
                remain = [o for i,o in enumerate(opts) if i not in ruled]
                if not remain:
                    # ä¸€ä¸ªéƒ½ä¸ç¬¦åˆ â†’ å›ç©º
                    self.bridge.sendline("")
                    self.pending_question = None
                    self.produce_step()
                    return True
                plan = self.llm.choose_or_ask(self.chat_history, remain) or {}
                act = plan.get("action")
                if act == "decide" and isinstance(plan.get("idx"), int) and 0 <= plan["idx"] < len(remain):
                    # æ˜ å°„å›åŸç´¢å¼•
                    decided_text = remain[plan["idx"]]
                    idx_global = opts.index(decided_text)
                    self.bridge.sendline(str(idx_global))
                    self.pending_question = None
                    self.produce_step()
                    return True
                if act == "ask_yn" and isinstance(plan.get("option_idx"), int):
                    target_text = remain[plan["option_idx"]]
                    idx_global = opts.index(target_text)
                    q = plan.get("question") or f"æ˜¯å¦å­˜åœ¨ä»¥ä¸‹ç°è±¡ï¼š{target_text}ï¼Ÿï¼ˆæ˜¯/å¦ï¼‰"
                    self.pending_question["target_idx"] = idx_global
                    self.pending_question["ruled_out"] = ruled
                    print("ğŸ¤– " + q)
                    self.logger.log("ASSIST", f"æé—®ï¼ˆYNç¡®è®¤ï¼‰ï¼š{q}")
                    return True
                if act == "ask_open":
                    q = plan.get("question") or "èƒ½å†å…·ä½“æè¿°ä¸€ä¸‹æ•…éšœç»†èŠ‚å—ï¼Ÿ"
                    self.pending_question = {"type":"open_probe", "options": remain}
                    print("ğŸ¤– " + q)
                    self.logger.log("ASSIST", f"æé—®ï¼ˆå¼€æ”¾è¿½é—®ï¼‰ï¼š{q}")
                    return True
                # é»˜è®¤ï¼šç»™ä¸€ä¸ªçŸ­æ¸…å•
                short = remain[:4]
                msg = f"{self.cfg.polite_prefix}ä»¥ä¸‹å“ªä¸€é¡¹æ›´æ¥è¿‘ï¼Ÿ\n" + \
                    "\n".join([f"{i}. {opt}" for i, opt in enumerate(short)])
                msg += "\nï¼ˆå¯å›å¤ç¼–å·æˆ–å…³é”®å­—ï¼‰"
                self.pending_question = {"type":"select", "options": short}
                print("ğŸ¤– " + msg)
                self.logger.log("ASSIST", "æé—®ï¼ˆå•é€‰/çŸ­æ¸…å•ï¼‰ï¼š\n" + msg)
                return True

        if kind == "open_probe":
            # æŠŠç”¨æˆ·æ–°æè¿°å¹¶å…¥ä¸Šä¸‹æ–‡ï¼Œå†è‡ªåŠ¨æ¨è¿›
            self.pending_question = None
            # æ–°æè¿°å¯èƒ½è®© LLM ç›´æ¥èƒ½å†³å®š
            self.produce_step()
            return True


        return False

# === main ================================================================
def main():
    cfg = Config()
    orch = Orchestrator(cfg)
    orch.run_cli()

if __name__ == "__main__":
    main()
